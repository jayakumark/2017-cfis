{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "So far we have learned how to train models for image classification, and we have evaluated their performance in terms of accuracy. But, what did these models learn? Let's try to understand that."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Filter visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "We will first use the model that we trained in the previous session and we will visualize the weights of the first convolutional layer. You can either use the model you trained on CIFAR in the previous session, or load the model in ```models/pretrained_cifar10.h5```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import time\n",
    "\n",
    "model = load_model('../models/pretrained_cifar10.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# get the weights from the first layer\n",
    "weights = model.layers[0].get_weights()[0] # [0] to get weights and not biases\n",
    "weights.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt  \n",
    "%matplotlib inline\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def display_filters(weights):\n",
    "    N = int(np.ceil(np.sqrt(weights.shape[3])))\n",
    "    f, axarr = plt.subplots(N, N)\n",
    "\n",
    "    p = 0\n",
    "    for i in range(N):\n",
    "        for j in range(N):\n",
    "\n",
    "            # empty plot white when out of kernels to display\n",
    "            if p >= weights.shape[3]:\n",
    "                krnl = np.ones((weights.shape[0],weights.shape[1],3))\n",
    "            else:\n",
    "                if weights.shape[2] == 3: \n",
    "                    # rgb filters converted to gray\n",
    "                     #krnl = 0.21*weights[:,:,0,p] + 0.72*weights[:,:,1,p] + 0.07*weights[:,:,2,p] \n",
    "                    krnl = weights[:,:,:,p]\n",
    "                    axarr[i,j].imshow(krnl)\n",
    "                else:\n",
    "                    krnl = np.mean(weights[:,:,:,p],axis=2)\n",
    "                    axarr[i,j].imshow(krnl,cmap='gray')\n",
    "            axarr[i,j].axis('off')\n",
    "            p+=1\n",
    "    plt.show()\n",
    "\n",
    "display_filters(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    " Do you see any patterns? Maybe the filters don't say much, but we will probably understand this better if we display the activations of this filters when convolved with an an image. We will not do this right now, but it is something that you are welcome to try after the session, or on your own time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Repeat the same experiment with filters and activations from the second convolutional layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "i = 2 # position of the second conv layer in the network\n",
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Activations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We can also use the model's activations to our data samples to understand what the model learned. Let's first display the model to know the name of the layer we want to extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Set ```layer_name``` to the name of the last fully connected layer before the classifier. We will use this layer as a feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "layer_name = '...'\n",
    "extractor = Model(input=model.input, output=model.get_layer(layer_name).output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have our extractor, we can load the data and forward it through the network to get the activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feats = extractor.predict(X_test, batch_size=32, verbose=0)\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Finding per unit top K samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's now find the K images with highest activation for each neuron in the layer, using the original extracted activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "K = 10\n",
    "idxs_top10 = np.argsort(feats,axis=0)[::-1][0:K,:]\n",
    "\n",
    "picked_samples = X_test[idxs_top10]\n",
    "picked_samples.shape\n",
    "#(n_images,n_units,nb_rows,nb_cols,nb_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```picked_samples``` now contains the 10 images with highest activation for each neuron. Let's visualize these images for some neurons:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Choose units to display their top K images. Do all units respond to distinguishable concepts? Are there units that respond to similar things?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "units = [i,j,k,...] # to be completed\n",
    "nunits = len(units)\n",
    "ims = picked_samples[:,units,:,:].squeeze()\n",
    "\n",
    "def vis_topk(ims,units):\n",
    "    f, axarr = plt.subplots(ims.shape[0],ims.shape[1],figsize=(10,10))\n",
    "    \n",
    "    for i in range(ims.shape[0]):\n",
    "        for j in range(ims.shape[1]):\n",
    "\n",
    "            axarr[i,j].imshow(ims[i,j,:,:,:])\n",
    "            axarr[i,j].axis('off')\n",
    "            axarr[0,j].set_title('unit '+ str(units[j]))\n",
    "            \n",
    "vis_topk(ims,units)\n",
    "ims.shape\n",
    "#(n_ims,n_units_picked,nb_rows,nb_cols,nb_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** Did you find any units with semantic meaning? You can try for different units and see what images they like the most."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### 1.3. Occlusion experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now, can we find what image parts contribute to the activation the most? Let's create a NxN occluder and slide it through each image with a stride of 2, and feed each occluded image through the network. Then, we can obtain the difference between the activations between the original image and the occluded ones, and create a difference map that we can use as a mask on top of the image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def occ_images(ims,occ=(11,11),stride=4):\n",
    "    \n",
    "    import copy\n",
    "    \n",
    "    # Reshape to put top images for all units stacked together\n",
    "    ims = np.rollaxis(ims,1,0)\n",
    "    ims = np.reshape(ims,(ims.shape[0]*ims.shape[1],ims.shape[2],ims.shape[3],ims.shape[4]))\n",
    "    ims_acc = ims\n",
    "    \n",
    "    # slide \n",
    "    npos = 1\n",
    "    st = int(np.floor(occ[0]/2))\n",
    "    \n",
    "    # slide occluder, set pixels to 0 and stack to matrix\n",
    "    for i in range(st,ims.shape[1],stride):\n",
    "        for j in range(st,ims.shape[2],stride):\n",
    "            ims_occ = copy.deepcopy(ims)\n",
    "            ims_occ[:,i-st:i+occ[0]-st,j-st:j+occ[1]-st,:] = 0\n",
    "            ims_acc = np.vstack((ims_acc,ims_occ))\n",
    "            npos+=1\n",
    "            \n",
    "    return ims_acc\n",
    "\n",
    "ims_acc = occ_images(ims)\n",
    "print (ims_acc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's visualize some of the images with the occluded region in different positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(10, 10,figsize=(10,10))\n",
    "ims_acc_r = np.reshape(ims_acc,(int(ims_acc.shape[0]/(ims.shape[0]*ims.shape[1])),\n",
    "                                ims.shape[1],ims.shape[0],\n",
    "                                ims_acc.shape[1],ims_acc.shape[2],ims_acc.shape[3]))\n",
    "for i in range(10):\n",
    "    for j in range(10):\n",
    "        axarr[i,j].imshow(ims_acc_r[i,0,j,:,:,:])\n",
    "        axarr[i,j].axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We should pick an occluder that is large enough to cover significant parts of objects. 11x11 is the default one, but you can experiment with other sizes and see their effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "```ims_occ``` contains all images with the occluder set at different positions. Let's run these through our extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feats = extractor.predict(ims_acc, batch_size=32, verbose=0)\n",
    "feats.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Now that we have the features, we can compute the difference between the original activation and the activation for each of the occluded images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feats_r = np.reshape(feats,(int(feats.shape[0]/(ims.shape[0]*ims.shape[1])),\n",
    "                                ims.shape[1],ims.shape[0],feats.shape[1]))\n",
    "\n",
    "distances = feats_r[0] - feats_r[1:] # original activation minus all the occluded ones\n",
    "distances = np.rollaxis(distances,0,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Reshaping the distance array into a 2D map will give a mask that we can display on top of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "s = int(np.sqrt(distances.shape[3]))\n",
    "\n",
    "heatmaps = np.zeros((distances.shape[0],distances.shape[1],distances.shape[3]))\n",
    "for i in range(len(units)):    \n",
    "    heatmaps[i] = distances[i,:,units[i],:]\n",
    "heatmaps.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Let's write a function to display the masks on top of the images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def vis_occ(ims,heatmaps,units,th=0.5,sig=2):\n",
    "    \n",
    "    from scipy.ndimage.interpolation import zoom\n",
    "    from scipy.ndimage.filters import gaussian_filter\n",
    "    import copy\n",
    "    \n",
    "    ims = np.rollaxis(ims,1,0)\n",
    "    \n",
    "    s = int(np.sqrt(heatmaps.shape[2]))\n",
    "    heatmaps = np.reshape(heatmaps,(heatmaps.shape[0],heatmaps.shape[1],s,s))\n",
    "    \n",
    "    f, axarr = plt.subplots(ims.shape[1],ims.shape[0],figsize=(10,10))\n",
    "    \n",
    "    for i in range(ims.shape[0]):\n",
    "        for j in range(ims.shape[1]):\n",
    "            \n",
    "            im = copy.deepcopy(ims[i,j,:,:,:])\n",
    "            mask = copy.deepcopy(heatmaps[i,j,:,:])\n",
    "            \n",
    "            # Normalize mask\n",
    "            mask = (mask - np.min(mask))/(np.max(mask)-np.min(mask))\n",
    "            # Resize to image size\n",
    "            mask = zoom(mask,float(im.shape[0])/heatmaps.shape[-1],order=1)\n",
    "            # Apply gaussian to smooth output\n",
    "            mask = gaussian_filter(mask,sigma=sig)\n",
    "            # threshold to obtain mask out of heatmap\n",
    "            mask[mask>=th] = 1\n",
    "            mask[mask<th] = 0.3\n",
    "            \n",
    "            # Mask all image channels\n",
    "            for c in range(3):\n",
    "                im[:,:,c] = im[:,:,c]*mask\n",
    "                \n",
    "            axarr[j,i].imshow(im)\n",
    "            axarr[j,i].axis('off')\n",
    "            axarr[0,i].set_title('unit '+ str(units[i]))\n",
    "            \n",
    "vis_occ(ims,heatmaps,units,th=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise**: The obtained masks are of course not perfect, but we get to see what parts of the image are most relevant for each unit in the layer. In some cases though we get quite accurate segmentations of objects. Are these masks what you expected? Do the picked neurons maximally respond to what you have previously guessed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### t-SNE (if there is enough time)\n",
    "\n",
    "**Exercise:** The first thing we can do is display our learned features in a 2D space using t-SNE. To do this, you can use the provided function in [scikit-learn](http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html). You can use the provided example as reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "t = time.time()\n",
    "\n",
    "# should do more iterations, but let's do the minimum due to time constraints\n",
    "n_iter = 400\n",
    "tsne = TSNE(n_components=2,random_state=0,n_iter=n_iter)\n",
    "feats_2d = tsne.fit_transform(feats)\n",
    "\n",
    "print (time.time() - t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Once we have our 2D features, we can display them with their class labels, to see if the learned features are discriminative enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#cifar_labels = ['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\n",
    "# 0: airplane\n",
    "# 1: automobile\n",
    "# 2: bird\n",
    "# 3: cat\n",
    "# 4: deer\n",
    "# 5: dog\n",
    "# 6: frog\n",
    "# 7: horse\n",
    "# 8: ship\n",
    "# 9: truck\n",
    "\n",
    "plt.scatter(feats_2d[:,0],feats_2d[:,1],c=y_test,cmap=plt.cm.get_cmap(\"jet\", 10),s=5) # 10 because of the number of classes\n",
    "plt.colorbar(ticks=range(10))\n",
    "plt.clim(-0.5, 9.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "**Exercise:** What categories seem to be easier for our model? Which ones are confusing?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
